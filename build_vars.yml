---
###
# See the documentation for details
###

###
## Binary file locations
###
## Parameter to specify the location of the tar.gz files to be unzipped.
## REQUIRED when executing the unzip operation. There is no default.
## Uncomment and set to the appropriate value before running unzip.
# nuage_zipped_files_dir: "/home/caso/nfs-data/5.2.2/nuage-packed"

## Parameter to specify the location of the unzipped binary files,
## e.g. *.qcow2, *.ova, *.rpm, etc. This path is used both as the
## destination for the unzip operation and as the location of binary
## files for the rest of the operations. It is a required parameter
## for all operations.
nuage_unzipped_files_dir: "/home/caso/nfs-data/5.2.2/nuage-unpacked"

## Parameter to specify the location for backups during upgrade. 
## The default value is nuage_unzipped_files_dir + "/backups".
## Uncomment and set to desired value for backup.
# metro_backup_root: "/home/caso/nfs-data/5.2.2/nuage-unpacked/backups"

###
## upgrade parameters
###
## Parameter to signal that the data in this file is for upgrade. By default, 
## the value is set to False to indicate that the data in this file is for
## install. Uncomment and set to True if this is an upgrade.
# nuage_upgrade: False


## VSTAT UPGRADE ONLY!
## Parameter to specify the pre-upgrade version of VSD software. The
## default value is 'NONE'. This is only required for VSTAT upgrade.
## Uncomment and set to the existing version if VSTAT is being upgraded.
# upgrade_from_version: '4.0.11'
# upgrade_to_version: '5.2.1'

## VSTAT UPGRADE ONLY! 
# Required only when upgarding to 4.0.11 versions and below
## NFS export that will be mounted on the VSTAT for backup and restore.
## Includes the server ip with the folder path. The default is 'NONE'
## Uncomment and provide an NFS export to mount if VSTAT upgrade.
# vstat_nfs_server_with_folder: 0.0.0.0:/tmp/vstat/

###
## SSH Public Key file
###
## Path to the public key file to be injected into the VMs being
## installed to enable passwordless SSH access. The default value is the
## default public key file for the current user. Uncomment and
## modify to specify a different value.
# user_ssh_pub_key: '~/.ssh/id_rsa.pub'

### 
## Location for health reports
###
## Path to the location where health reports are to be written.
## Default value is 'reports' under the playbook directory.
## Unccomment and modify if a different location is required.
# metro_reports_dir: "{{ playbook_dir }}/reports"

###
## Hypervisor network bridges
###
## Network bridges required on the target server (hypervisor) for VM deployment.
## On KVM, the bridges must exist as specified. The VSD needs mgmt_bridge. The
## VSC needs mgmt_bridge and data_bridge. nsgv requires the access_bridge. On
## vcenter, mgmt_bridge specifies the portgroup in vCenter to which the
## management interfaces need to be connected to, and data_bridge specifies the
## portgroup in vCenter to which the data interfaces need to be connected to.
## In vCenter/ESXi, you have virtual switches and portgroups. A virtual switch
## is similar to a normal switch, a network separation object. VMs on different
## vSwitches can not communicate directly with each other, they have to leave
## the host through the physical uplinks of the vSwitch, up to the physical
## network, to connect to each other (if the physical network is configured
## correctly). A portgroup is a sub section of a vSwitch that groups ports, this
## group of ports have the same configuration (for instance, the same VLAN, same
## security settings, …). A port can only be part of one portgroup, a VM can of
## course have multiple interfaces on multiple portgroups (and a vm can have
## interfaces on portgroups that are on different vSwitches as well).
## Note that these are global network settings. Depending on the usecase, these
## variables can be defined in the specific components (myvsds, myvscs etc). If
## defined, they will overwrite the global settings.
mgmt_bridge: "virbr0"
data_bridge: "virbr1"
access_bridge: "access"

###
## KVM Hypervisor VM image file location
###
## Location to create VM images on KVM target servers
## Default location is `/var/lib/libvirt/images' on the KVM system.
## Uncomment and modify if a different path is desired.
# images_path: "/var/lib/libvirt/images/"

###
## NTP configuration
###
## IP addresses of up to 3 NTP servers to sync all components with. There is no default.
## The values must be specified in dotted-decimal format
ntp_server_list:
  - 135.227.181.232
  - 128.138.141.172
###
## timezone for NTP configuration on all devices. Default is 'US/Pacific'. Uncomment
## and modify if a different setting is required.
# timezone: US/Pacific

###
## DNS configuration
###
## IP addresses of up to 3 DNS servers to configure all components with. There is no default.
## Required The values must be specified in dotted-decimal format
dns_server_list:
  - 192.168.122.1
  - 128.251.10.145
###
## The name of the domain for all components. There is no default. Required.
dns_domain: example.com

###
## Misc. params
###
## yum_proxy is set to the address of your internal yum proxy server if you don't
## have internet access. The default value is 'NONE' which means it won't be used.
## if you have  yum proxy, iunncomment and update the value to the address of 
## the server.
# yum_proxy: "NONE"
## yum_update is used to determine if a yum update should be done on several components
## as part of the installation. By default, it is set to 'True', the recommended value.
## Uncomment and set to 'False' if you want to skip the yum update--acceptable only in
## lab environments.
# yum_update: True
## secure_communication is used to setup TLS on all the communication between the VSD, VSC, 
## VRS and NSGV. By default, it is set to 'True', the recomemded value.
## Uncomment and set to 'False' if you don't want to use TLS
#secure_communication: True

###
## Global Vcenter params
##
## These are global vcenter credentials that are used *only* when deploying 
## on a Vmware environment. They be ignored when deploying on other hypervisors,
## e.g. KVM. Uncomment and update the values if you are using vcenter.
##
## Note that this set of vcenter credentials is used on *all* vcenter deployments
## *unless* you specify a specific set for a given component. See individiual
## components, e.g. VSD, for more detail.
##
## username
## The vCenter user with proper permissions to deploy an OVA on the specified
## cluster (at least: create VM, configure VM, boot VM, Allocate disk space,
## connect networks. There might be other permissions required)
##
## password
## The password with which to connect to vCenter, matching the user specified in
## the username field
##
## datacenter
## The vCenter datacenter in which the cluster is located, a Datacenter is the
## first level of objects beneath the root vCenter object
##
## cluster
## The vCenter cluster where the VMs need to be deployed in. A cluster is a
## grouping of ESXi hosts with more or less the same purpose. In a typical
## datacenter deployment, you will see a management cluster, a grouping of ESXi
## hosts where all management components run (for instance vCenter, Active
## Directory, DNS/NTP servers, …); and one or more compute or customer clusters,
## a grouping of ESXi hosts with the purpose of handling non-management VMs
## (compute nodes)
##
## datastore
## The vCenter datastore where the VMs need to be located. A datastore is a set
## of disks (local storage), an NFS server or an iSCSI/FiberChannel  LUN which is
## exposed to one or more ESXi hosts. In case a datastore is specified to which
## only one ESXi host is connected (for instance in case of local storage), the
## VM that is being deployed, will only be able to run on that specific ESXi
## host. If the datastore is available on multiple ESXi hosts in the specified
## cluster (in case of shared storage like NFS, iSCSI or FC), when the VM is
## booted, vCenter will pick a host to boot the VM on (random, or based on the
## load of the cluster, depending on the cluster configuration). Datastores can
## be renamed… by default, if a local datastore is created on an ESXi host of
## the local disks, it get the name 'datastore', in case multiple ESXi hosts are
## added to vCenter and each has a local datastore called 'datastore', they will
## get a follow number ('datastore (1)', 'datastore (2)', …). It is recommended
## that you rename the local datastore from the default to the same name as the
## host it is from, that clearly identifies the local datastore of a specific
## ESXi host.  In case a datastore is shared (NFS, iSCSI, FC) on multiple hosts,
## it will only show up once in vCenter, but the hosts tab will show it is
## available on multiple hosts (view in the screenshot below)
##
## resource_pool
## The vCenter resource pool where the VMs need to be located. A resource pool
## is a logical abstraction of resources. Different resource pools can be 
## configured to have different priorities in case of resource contention and 
## can have different resource reservations and limitations.
## In a typical deployment, you will see a resource pool with a high number of
## shares (higher priority) which will be used for the important components of
## Nuage, like the VSD and VSC's.
##
## ovftool
## Binary location of the ovftool
##
## There are no default values for vcenter credentials. Uncomment and update
## when running in a vcenter environment.
# vcenter:
#   username: administrator@vsphere.local
#   password: Alcateldc
#   datacenter: Datacenter
#   cluster: Management
#   datastore: Datastore
#   resource_pool: Resourece Pool 
#   ovftool: /usr/bin/ovftool

###
## VSD params
###
## vsd_fqdn_global is required when this file includes myvsds. It is not required
## if you aren't operating on VSDs--it will be ignored when 'myvsds' is not defined.
## There is no default.
## Use xmpp fqdn for clustered VSDs or the vsd fqdn for stand alone
vsd_fqdn_global: "vsd1.nuage.met"
###
## vsd_operations_list is required when this file includes myvsds. It is not required
## if you aren't operating on VSDs--it will be ignored when 'myvsds' is not defined.
## This list must include exactly 1 of the following:
## - install
## - upgrade
## - health
## There is no default value.
vsd_operations_list:
  - install
###
## myvsds is required when you are operating on VSDs. It is not required if you aren't
## operating on VSDs--it will be ignored if not defined. Each element in the list
## is a dictionary of parameters specific to a single VSD. You are required to define
## 0, 1, or 3 VSDs in the list. No other number of VSDs is supported.
##
## hostname
## Required always: The FQDN or IP address of the VSD management port
##
## vmname
## Optional for install, required for upgrade and health. For install,
## vmname defaults to the hostname. Uncomment and set if doing an
## upgrade or you want a VM name other than hostname.
##
## upgrade_vmname
## Required for upgrade, ignored otherwise. This is the name used for the
## new VSD in an upgrade. Uncomment and set if doing an upgrade.
##
## target_server_type
## Required: The type of hypervisor the VSD will be deployed on. Supported values
## are kvm, vcenter, and heat. 
##
## target_server
## Required: The hostname or IP address of the hypervisor where this VSD will be
## instantiated.
##
## mgmt_bridge
## Optional: The name of the bridge on the hypervisor to connect the mgmt port to.
## By default, the mgmt port will be connected to the global mgmt_bridge that is
## defined elsewhere in this file. Uncomment and update if you want to use a
## different bridge for this component.
##
## mgmt_ip
## Required: The IP address of the VSDs management port.
##
## mgmt_gateway
## Required: The IP address for the default gateway.
##
## mgmt_netmask
## Required: The netmask for the management port.
##
## The example, below, is for a 3-VSD cluster. If deploying stand-alone,
## only one VSD defintion is required.
myvsds:
  - { hostname: "vsd1.nuage.met",
#      vmname: vsd1,
#      upgrade_vmname: vsd1-new,
      target_server_type: "kvm",
      target_server: 135.227.181.233,
#      mgmt_bridge: br0,
      mgmt_ip: 192.168.122.201,
      mgmt_gateway: 192.168.122.1,
      mgmt_netmask: 255.255.255.0 }
  - { hostname: "vsd2.nuage.met",
#      vmname: vsd2,
#      upgrade_vmname: vsd2-new,
      target_server_type: "kvm",
      target_server: 135.227.181.233,
#      mgmt_bridge: br1,
      mgmt_ip: 192.168.122.202,
      mgmt_gateway: 192.168.122.1,
      mgmt_netmask: 255.255.255.0 }
  - { hostname: "vsd3.nuage.met",
#      vmname: vsd3,
#      upgrade_vmname: vsd3-new,
      target_server_type: "kvm",
      target_server: 135.227.181.233,
#      mgmt_bridge: br2,
      mgmt_ip: 192.168.122.203,
      mgmt_gateway: 192.168.122.1,
      mgmt_netmask: 255.255.255.0 }

###
## VSC params
###
## vsc_operations_list is required when this file includes myvscs. It is not required
## if you aren't operating on VSCs--it will be ignored when 'myvscs' is not defined.
## This list must include exactly 1 of the following:
## - install
## - upgrade
## - health
vsc_operations_list:
  - install
###
## myvscs is required when you are operating on VSCs. It is not required if you aren't
## operating on VSCs--it will be ignored if not defined. Each element in the list
## is a dictionary of parameters specific to a single VSC. You are required to define
## 0, 1, or more VSCs in the list.
##
## Note: Upgrade only supports 1 or 2 VSCs. If you have more than 2 VSCs, you must
## create multiple build_vars.yml files, one for each single VSC or each pair of
## VSCs, and upgrade them as single instances or in pairs. In other words, for an intall,
## you can specify as many VSCs in build_vars.yml as you want, but for upgrade to work
## you are limited to upgrading one or two at a time.
##
## hostname
## Required always: The FQDN or IP address of the VSC management port
##
## vmname
## Optional for install, required for upgrade and health. For install,
## vmname defaults to the hostname. Uncomment and set if doing an
## upgrade or you want a VM name other than hostname.
##
## upgrade_vmname
## Required for upgrade, ignored otherwise. This is the name used for the
## new VSD in an upgrade. Uncomment and set if doing an upgrade.
##
## target_server_type
## Required: The type of hypervisor the VSC will be deployed on. Supported values
## are kvm, vcenter, and heat.
##
## target_server
## Required: The hostname or IP address of the hypervisor where this VSC will be
## instantiated.
##
## mgmt_bridge
## Optional: The name of the bridge on the hypervisor to connect the mgmt port to.
## By default, the mgmt port will be connected to the global mgmt_bridge that is
## defined elsewhere in this file. Uncomment and update if you want to use a
## different bridge for this component.
##
## mgmt_ip
## Required: The IP address of the VSDs management port.
##
## mgmt_gateway
## Required: The IP address for the default gateway.
##
## mgmt_netmask_prefix
## Required: The netmask prefix for the management port network.
## 
## ctrl_ip
## Required: The IP address of the control port.
##
## ctrl_netmask_prefix
## Required: The netmask prefix for the control port network.
##
## vsd_fqdn
## Required: The FQDN of the VSD or VSD cluster this VSC will
## connect to. Use the XMPP server name for a cluster, the
## VSD name for stand-alone VSD.
##
## system_ip
## Required: The IP address to use for the system port
##
## expected_num_bgp_peers
## expected_num_vswitches
## expected_num_host_vports
## expected_num_vm_vports
## expected_num_gateway_ports
## Optional: Values to use for this VSC when running a health test. All values are
## set to 0 by default, which means they will be ignored. To use them, uncomment 
## and set to the expected values.
##
## vsc_mgmt_static_route_list
## Optional: The static routes to create for the mgmt port. They will use the values
## '[ 0.0.0.0/1, 128.0.0.1/1 ]' by default. To use a different set of routes, uncomment
## and modify to suit.
##
## xmpp_username
## Required: The username to establish for the XMPP connection to VSD.
##
## The example, below, is for a single VSC pair.
myvscs:
  - { hostname: "vsc1.nuage.met",
#      vmname: vsc1,
      target_server_type: "kvm",
      target_server: 135.227.181.233,
#      mgmt_bridge: br1,
      mgmt_ip: 192.168.122.204,
      mgmt_gateway: 192.168.122.1,
      ctrl_ip_and_prefix: 192.168.100.202/24,
      vsd_fqdn: "{{ vsd_fqdn_global }}",
      system_ip: 1.1.1.2,
#      expected_num_bgp_peers: 0 ,
#      expected_num_vswitches: 0 ,
#      expected_num_host_vports: 0,
#      expected_num_vm_vports: 0,
#      expected_num_gateway_ports: 0,
#      vsc_mgmt_static_route_list: [ 0.0.0.0/1, 128.0.0.1/1 ],
#      secure_communication: "{{ secure_communication }}",
      xmpp_username: vsc1}
  - { hostname: "vsc1.nuage.met",
#      vmname: vsc1,
      target_server_type: "kvm",
      target_server: 135.227.181.233,
#      mgmt_bridge: br1,
      mgmt_ip: 192.168.122.205,
      mgmt_gateway: 192.168.122.1,
      ctrl_ip_and_prefix: 192.168.100.203/24,
      vsd_fqdn: "{{ vsd_fqdn_global }}",
      system_ip: 1.1.1.3,
#      expected_num_bgp_peers: 0 ,
#      expected_num_vswitches: 0 ,
#      expected_num_host_vports: 0,
#      expected_num_vm_vports: 0,
#      expected_num_gateway_ports: 0,
#      vsc_mgmt_static_route_list: [ 0.0.0.0/1, 128.0.0.1/1 ],
#      secure_communication: "{{ secure_communication }}",
      xmpp_username: vsc2}
###
## VRS params
###
## libnetwork
##
## These parameters are for installing libnetwork support on the VRS nodes. These
## are only required when you have defined 'libnetwork_install: True' in one or
## more VRS sets in the myvrss section elsewhere in this file. These parameters
## are required when libnetwork is to be installed. They are optional when
## not installing libnetwork. Uncomment and modify as needed if libnetwork is
## to be installed on the VRS nodes.
##
## 'scope: local' means that connectivity between containers is limited to within
## the local host.
## 'scope: global' means that connectivity between containers may span across
## hosts in the cluster.
## cluster_store_url must point to the key-value store used for the cluster
#libnetwork:
#  scope: local
#  cluster_store_url: consul://192.168.122.1:8500
###
## vrs_operations_list is required when this file includes myvrss. It is not required
## if you aren't operating on VRSs--it will be ignored when 'myvrss' is not defined.
## This list must include exactly 1 of the following:
## - install
## - health
vrs_operations_list:
  - install
###
## myvrss is required when you are operating on VRSs. It is not required if you aren't
## operating on VRSs--it will be ignored if not defined. Each element in the list
## is a dictionary of parameters specific to a single group of VRSs. You are required to define
## 0, 1, or more VRSs in the list.
##
## vrs_os_type
## Required: the OS running on the compute node the VRSs will be installed on. It must be
## one of the following: 'u14.04', 'u16.04', or 'el7'. u14.04 = Ubuntu version 14.04.
## u16.04 == Ubuntu version 16.04. el7 == CentOS or RHEL version 7.
##
## libnetwork_install
## Optional: Whether to install libnetwork. Default is False. If installing
## libnetwork on all the nodes in this set, uncomment and set to True.
##
## dkms_install
## Optional: Whether to install dkms. Default is False. If installing
## dkms on all the nodes in this set, uncomment and set to True.
##
## active_controller_ip
## standby_controller_ip
## Required: The IP addresses of the active and standby controllers (VSCs) for
## every VRS being installed in this set.
##
## vrs_ip_list
## Required: A list of the IP addresses for the compute nodes in this VRS group.
## The assumption is that VRS will be installed on every IP address listed here,
## and every compute node listed is running the OS listed in vrs_os_type, above.
##
## The example, below, will install VRS on 5 compute nodes: 1 on Ubuntu 14.04, 
## 2 on el7, and 1 on Ubuntu 16.04
myvrss:
  - { vrs_os_type: u14.04,
#      libnetwork_install: False,
#      dkms_install: False,
      secure_openFlow: true,
      active_controller_ip: 192.168.122.204,
      standby_controller_ip: 192.168.122.205,
#      secure_communication: "{{ secure_communication }}",
      vrs_ip_list: [
        192.168.122.101] }
  - { vrs_os_type: el7,
#      libnetwork_install: False,
#      dkms_install: False,
      active_controller_ip: 192.168.122.204,
      standby_controller_ip: 192.168.122.205,
#      secure_communication: "{{ secure_communication }}",
      vrs_ip_list: [
        192.168.122.83,
        192.168.122.238 ] }
  - { vrs_os_type: u16.04,
#      libnetwork_install: False,
#      dkms_install: False,
      active_controller_ip: 192.168.122.204,
      standby_controller_ip: 192.168.122.205,
#      secure_communication: "{{ secure_communication }}",
      vrs_ip_list: [
        192.168.122.215 ] }
###
## Stats VM (ElasticSearch) params
###
## vstat_operations_list is required when this file includes myvstats or mydnss. It is not required
## if you aren't operating on VSTATSs or DNSs--it will be ignored when 'myvstats' and 'mydnss' are not defined.
## This list must include exactly 1 of the following:
## - install
## - upgrade
## - health
## - dns (specified when deploying the VSTAT image as a DNS server)
vstat_operations_list:
  - install
###
## myvstats is required when you are operating on VSTATs. It is not required if you aren't
## operating on VSTATs--it will be ignored if not defined. Each element in the list
## is a dictionary of parameters specific to a single VSTAT. You are required to define
## 0, 1, or 3 VSTATs in the list. No other number of VSTATs is supported.
##
## hostname
## Required always: The FQDN or IP address of the VSTAT management port
##
## vmname
## Optional for install, required for upgrade and health. For install,
## vmname defaults to the hostname. Uncomment and set if doing an
## upgrade or you want a VM name other than hostname. Note: Some VSP
## upgrades do not require an update of the VSTAT.
##
## upgrade_vmname
## Required for upgrade, ignored otherwise. This is the name used for the
## new VSTAT in an upgrade. Uncomment and set if doing an upgrade. Note: Some VSP
## upgrades do not require an update of the VSTAT.
##
## target_server_type
## Required: The type of hypervisor the VSTAT will be deployed on. Supported values
## are kvm, vcenter, and heat.
##
## target_server
## Required: The hostname or IP address of the hypervisor where this VSTAT will be
## instantiated.
##
## mgmt_bridge
## Optional: The name of the bridge on the hypervisor to connect the mgmt port to.
## By default, the mgmt port will be connected to the global mgmt_bridge that is
## defined elsewhere in this file. Uncomment and update if you want to use a
## different bridge for this component.
##
## mgmt_ip
## Required: The IP address of the VSDs management port.
##
## mgmt_gateway
## Required: The IP address for the default gateway.
##
## mgmt_netmask
## Required: The netmask for the management port.
##
## vsd_fqdn
## Required: The FQDN of the VSD or VSD cluster this VSC will
## connect to. Use the XMPP server name for a cluster, the
## VSD name for stand-alone VSD.
##
## The example, below, is for a 3-VSTAT cluster. If deploying stand-alone,
## only one VSD defintion is required.
myvstats:
  - { hostname: "vstat1.nuage.met",
#      vmname: vstat1,
#      upgrade_vmname: vstat1-new,
      target_server_type: "kvm",
      target_server: 135.227.181.233,
#      mgmt_bridge: bridge1,
      mgmt_ip: 192.168.122.206,
      mgmt_gateway: 192.168.122.1,
      mgmt_netmask: 255.255.255.0,
      vsd_fqdn: "vsd1.nuage.met" }
  - { hostname: "vstat1.nuage.met",
#      vmname: vstat1,
#      upgrade_vmname: vstat1-4.0.R10-dc1,
      target_server_type: "kvm",
      target_server: 135.227.181.233,
#      mgmt_bridge: bridge2,
      mgmt_ip: 192.168.122.207,
      mgmt_gateway: 192.168.122.1,
      mgmt_netmask: 255.255.255.0,
      vsd_fqdn: "vsd1.nuage.met" }
  - { hostname: "vstat3.nuage.met",
#      vmname: vstat3,
#      upgrade_vmname: vstat3-new,
      target_server_type: "kvm",
      target_server: 135.227.181.233,
#      mgmt_bridge: bridge3,
      mgmt_ip: 192.168.122.208,
      mgmt_gateway: 192.168.122.1,
      mgmt_netmask: 255.255.255.0,
      vsd_fqdn: "vsd1.nuage.met" }

###
## VNS params
###
## vns_operations_list is required when this file includes myvnsutils or mynsgvs. It is not required
## if you aren't operating on VNS componentss--it will be ignored when 'myvnsutils' and 'mynsgvs'
## are not defined.
## This list must include exactly 1 of the following:
## - install
## - health
vns_operations_list:
  - install
###
## myvnsutils is required when you are operating on VNSUTILs. It is not required if you aren't
## operating on VNSUTILs--it will be ignored if not defined. Each element in the list
## is a dictionary of parameters specific to a single VNSUTIL. You are required to define
## 0, 1, or more VNSUTILs--although 1 is generally enough.
##
## hostname
## Required always: The FQDN or IP address of the VSTAT management port
##
## vmname
## Optional for install. Uncomment and set if desired.
##
## target_server_type
## Required: The type of hypervisor the VNSUTIL will be deployed on. Supported values
## are kvm, vcenter, and heat.
##
## target_server
## Required: The hostname or IP address of the hypervisor where this VNSUTIL will be
## instantiated.
##
## mgmt_bridge
## Optional: The name of the bridge on the hypervisor to connect the mgmt port to.
## By default, the mgmt port will be connected to the global mgmt_bridge that is
## defined elsewhere in this file. Uncomment and update if you want to use a
## different bridge for this component.
##
## mgmt_ip
## Required: The IP address of the VNSUTIL management port.
##
## mgmt_gateway
## Required: The IP address for the default gateway.
##
## mgmt_netmask
## Required: The netmask for the management port.
##
## data_fqdn
## Required: The FQDN of the VNSUTIL data port.
##
## data_ip
## Required: The IP address for the data port.
##
## data_netmask
## Required: The netmask for the data port network.
##
## data_static_route
## Optional: list of eth1 static routes to the data networks
##
## data_gateway 
## Optional: will be used as next-hop for the data_static_routes
##
## DHCP Bootstrap support
## Optional: Metro Automation Engine supports a special case, automatically bootstrapping
## (ZFB) a single NSGV at time of deployment of the VNS UTIL VM. To enable 
## this special case, the variables in this section must be uncommented and 
## defined so that Metro can configure the DHCP server on the VNS UTIL VM
## to participate in this process. If you are *not* going to use Metro's
## automatic bootstrap of a single NSGV, the variables in this section 
## must not be defined.
##
## Uncomment in the myvnsutils section for DHCP bootstrap support:
## data_subnet: 192.168.100.0,
## nsgv_ip: 192.168.100.206,
## nsgv_mac: '52:54:00:88:85:12',
## nsgv_hostname: "nsgv1.{{ dns_domain }}",
##
## vsd_fqdn
## Required: The FQDN of the VSD or VSD cluster this VSC will
## connect to. Use the XMPP server name for a cluster, the
## VSD name for stand-alone VSD.
##
## The example, below, is for a single VNSUTIL.
myvnsutils:
  - { hostname: "vnsutil1.nuage.met",
#      vmname: vnsutil1,
      target_server_type: "kvm",
      target_server: 135.227.181.233,
#      mgmt_bridge: bridge2,
      mgmt_ip: 192.168.122.209,
      mgmt_gateway: 192.168.122.1,
      mgmt_netmask: 255.255.255.0,
      data_fqdn: "vnsutil1.data.nuage.met",
      data_ip: 192.168.100.205,
      data_netmask: 255.255.255.0,
#      data_gateway: 192.168.100.1,
#      data_static_route: [ 192.168.99.0/24, 192.168.98.0/24, 1192.168.97.0/24 ],
#      data_subnet: 192.168.100.0,
#      nsgv_ip: 192.168.100.206,
#      nsgv_mac: '52:54:00:88:85:12',
#      nsgv_hostname: "nsgv1.{{ dns_domain }}",
#      secure_communication: "{{ secure_communication }}",
      vsd_fqdn: "{{ vsd_fqdn_global }}"
    }
###
## mynsgvs is required when you are operating on NSGVs. It is not required if you aren't
## operating on NSGVs--it will be ignored if not defined. Each element in the list
## is a dictionary of parameters specific to a single NSGV. You are required to define
## 0, 1, or more NSGVs.
##
## hostname
## Required always: The FQDN or IP address of the NSGV management port
##
## vmname
## Optional for install. If not specified, the hostname will be used as the VM name. Uncomment and set if desired.
##
## target_server_type
## Required: The type of hypervisor the NSGV will be deployed on. Supported values
## are kvm, vcenter, and heat.
##
## target_server
## Required: The hostname or IP address of the hypervisor where this NSGV will be
## instantiated.
##
## iso_path
## Optional: Only required when bootstrap_method id set to 'zfb_external'. It is the local
## path to the 3rd-party ISO file to use for bootstrap.
##
## iso_file
## Optional: Only required when bootstrap_method is set to 'zfb_external'. It is the name
## of the 3rd-party ISO file found on iso_path.
##
## nsgv_mac
## Optional: Only required when bootstrap_method is set to 'zfb_metro'. It is the mac
## address the VNSTUIL will use to recognize the NSGV at boot/DHCP time.
##
## bootstrap_method
## Optional: The default bootstrap method is 'none'. If another method is desired,
## uncomment and modify to suit. The values supported are:
## - zfb_metro
##     ZFB process supported by metro, needs zfb_vars.yml updated by user
## - zfb_external
##     ZFB process taken care by third party iso file. Needs two additional vars
##     iso_path, iso_file)
## - none (nsgv is deployed without any bootstrapping)
##
## The example, below, is for a single NSGV that uses no bootstrapping.
mynsgvs:
  - { hostname: "nsgv1.nuage.met",
#      vmname: nsgv1,
      target_server_type: "kvm",
#      iso_path: '/tmp/iso',
#      iso_file: 'user_img.iso',
#      nsgv_mac: '52:54:00:88:85:12',
#      bootstrap_method: none,
#      secure_communication: {{ secure_communication }},
      target_server: 135.227.181.233 }
###
## VCIN params
###
## vcin_operations_list = A list of the operations you intend for the VCIN. The
## list must include exactly 1 of the following:
## - install
## - upgrade
vcin_operations_list:
  - install
###
## myvcins is required when you are operating on VCINs. It is not required if you aren't
## operating on VCINs--it will be ignored if not defined. Each element in the list
## is a dictionary of parameters specific to a single VCIN. You are required to define
## 0, 1, or more VCINs in the list. Usually, only 1 VCIN is required.
##
## hostname
## Required always: The FQDN or IP address of the VCIN management port
##
## master_vcin
## Optional: The FQDN or IP address of the Master VCIN in an Active/Standby
## deployment. This must match the hostname of another VCIN in the list of
## myvcins. 
## Validation is in place to assure:
## - Masters can not be their own slave
## - A Master can only have one slave
## - The Master must be present when configured on a slave
##
## target_server_type
## Required: The type of hypervisor the VCIN will be deployed on. Supported values
## are kvm, vcenter, and heat.
##
## target_server
## Required: The hostname or IP address of the hypervisor where this VCIN will be
## instantiated.
##
## vcenter
## Optional: vcenter params to use of the global params, defined elsewhere in this 
## file, are not appropriate.
##
## mgmt_ip
## Required: The IP address of the VCIN management port.
##
## mgmt_gateway
## Required: The IP address for the default gateway.
##
## mgmt_netmask
## Required: The netmask for the management port.
##
## The example, below, is for a single VCIN. If deploying stand-alone,
## only one VSD defintion is required.
myvcins:
    - { hostname: vcin1.nuage.net,
#      master_vcin: vcin2.nuage.net,
      target_server_type: "vcenter",
      target_server: 135.227.181.232,
#      vcenter: { username: administrator@vsphere.local,
#        password: Alcateldc,
#        datacenter: Antwerp,
#        cluster: Management,
#        datastore: esxi02 },
      mgmt_ip: 10.189.1.246,
      mgmt_gateway: 10.0.0.1,
      mgmt_netmask: 255.0.0.0 }

###
## Utility VMs
###
## The rest of the items in this file are optional utility components that can be
## used if required. Uncomment and modify to suit. By default, all will be ignored.
## DNS params
## NOTE: This code assumes that the DNS image is the VSTAT/ElasticSearch image.
## For this code to work, myvstats must also be defined.
## dns_operations_list = A list of the operations you intend for the DNS. The
## list could include 1 or more of the following:
## - install
#dns_operations_list:
#  - install
###
## mydnss is a collection of parameters for the DNS.
## One set of parameters is required for each DNS.
## data_static_route list of eth1 static routes to the data networks
## data_gateway will be used as next-hop for the data_static_routes
## dns_mgmt_lookup list of management dns lookup
## dns_data_lookup list of data dns lookup
## dns_mgmt name server of managment network
## dns_data name server of data network
## dns_server dns server of this server
###
#mydnss:
#  - { hostname: g5dns.mgmt.training.net,
#      target_server_type: "kvm",
#      target_server: 10.167.53.5,
#      mgmt_ip: 10.167.53.3,
#      mgmt_gateway: 10.167.53.254,
#      mgmt_netmask: 255.255.255.0,
#      data_ip: 10.167.54.3,
#      data_subnet: 10.167.54.0,
#      data_netmask: 255.255.255.0,
#      data_gateway: 10.167.54.1,
#      data_static_route: [ 10.165.53.0/24, 10.165.54.0/24, 10.165.55.0/24 ],
#      dns_server: 8.8.8.8,
#      dns_mgmt: g5dns.mgmt.training.net.,
#      dns_data: g5dns.data.training.net.,
#      dns_mgmt_lookup: {g5dns.mgmt.training.net.: 10.166.53.3,
#        ns1.mgmt.training.net.: 10.166.53.2,
#        ns2.mgmt.training.net.: 10.166.53.2,
#        server1.mgmt.training.net.: 10.166.53.5,
#        server2.mgmt.training.net.: 10.166.53.6,
#        g1vsd.mgmt.training.net.: 10.166.53.11,
#        g1es.mgmt.training.net.: 10.166.53.12,
#        g1util.mgmt.training.net.: 10.166.53.13,
#        g1vsc1.mgmt.training.net.: 10.166.53.14,
#        g1vsc2.mgmt.training.net.: 10.166.53.15,
#        g1pe.mgmt.training.net.: 10.166.53.16,
#        g1ppsvm.mgmt.training.net.: 10.166.53.17,
#        g1ubuntu.mgmt.training.net.: 10.166.53.18,
#        g1cpe.mgmt.training.net.: 10.166.53.19,
#        g1utilinternet.mgmt.training.net.: 10.166.53.20,
#        g1uitlnokianet.mgmt.training.net.: 10.166.53.21},
#        dns_data_lookup: {g1utilnokianet.data.training.net.: 10.167.53.21,
#        g1utilinternet.data.training.net.: 10.167.54.20,
#        g1pe.data.training.net.: 10.167.54.16,
#        g1vsc2.data.training.net.: 10.167.54.15,
#        g1vsc1.data.training.net.: 10.167.54.14,
#        g1util.data.training.net.: 10.167.54.13,
#        g5dns.data.training.net.: 10.167.54.3,
#        server2.data.training.net.: 10.167.54.6,
#        server1.data.training.net.: 10.167.54.5,
#        ns1.data.training.net.: 10.167.54.3,
#        ns2.data.training.net.: 10.167.54.3}}
###
## Generic VM params
## gvm_operations_list = A list of the operations you intend for the vms. The
## list could include 1 or more of the following:
## - install
#gvm_operations_list:
#  - install
## mygvms is a collection of parameters for the generic vms.
## One set of parameters is required for each generic VM.
##
## gvm_xml_file is the full path to the XML file that defines the VM.
#mygvms:
#  - { hostname: "gvm1.{{ dns_domain }}",
#      target_server_type: "kvm",
#      target_server: 135.227.181.233,
#      gvm_xml_file: "/home/caso/images/jenkinsgvm1.{{ dns_domain }}/gvm.xml" }
###
## VSR params
## vsr_operations_list = A list of the operations you intend for the VSR. The
## list could include 1 or more of the following:
## - install
#vsr_operations_list:
#  - install
## myvsrs is a collection of parameters for the targets where VSR is to be
## deployed. One set of parameters is required for each set or group of VSR
## targets.
## mgmt_* - configuration of VSR bof interface
## ports - a list of VSR traffic ports (1/1/1, 1/1/2 and etc)
## license_file - absolute path to license file. Support zip and txt license
##   file.
## deploy_cfg_file - absolute path to file with configuration for vsr-deploy
##   role.
#myvsrs:
#  - { hostname: "vsr1.{{ dns_domain }}",
#      vmname: "vsr1",
#      target_server_type: "kvm",
#      target_server: 10.167.28.4,
#      mgmt_ip: 10.167.28.91,
#      mgmt_gateway: 10.167.28.1,
#      mgmt_netmask_prefix: 24,
#      mgmt_static_route_list: [ 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16 ],
#      mgmt_to_hv_bridge: 'br0',
#      ports_to_hv_bridges: ['br0', 'br1','br0','br1'],
#      license_file: '/path/on/ansible/deployment/host/license.zip',
#      deploy_cfg_file: '/path/on/ansible/deployment/host/config_flat.txt'}

## VRS-VM params
## vrs_vm_operations_list = A list of the operations you intend for the VRS-VM.
##The list could include 1 or more of the following:
## - install
## myvrs_vms is required when you are operating on VRS-VMs. It is not required if you aren't
## operating on VRS-VMs. It will be ignored if not defined. Each element in the list
## is a dictionary of parameters specific to a single VRS-VM. You can define as much
## VRS-VM as you want
##
## hostname
## Required always: The FQDN or IP address of the VRS-VM management port
##
## vmname
## Optional, vmname defaults to the hostname. Uncomment and set if you want a 
##VM name other than hostname.
##
## target_server_type
## Optional: The default is 'kvm'. For now only KVM is supported.
##
## target_server
## Required: The hostname or IP address of the hypervisor where this VRS-VM will be
## instantiated.
##
## mgmt_bridge
## Optional: The name of the bridge on the hypervisor to connect the mgmt port to.
## By default, the mgmt port will be connected to the global mgmt_bridge that is
## defined elsewhere in this file. Uncomment and update if you want to use a
## different bridge for this component.
##
## mgmt_ip
## Required: The IP address of the VRS-VM's management port.
##
## mgmt_gateway
## Required: The IP address for the default gateway.
##
## mgmt_netmask
## Required: The netmask for the management port.
## 
## data_ip
## Required: The IP address of the data plane.
##
## data_netmask
## Required: The netmask for the data plane network.
##
## data_bridge
## Optional: The name of the bridge on the hypervisor to connect the data port to.
## By default, the data port will be connected to the global data_bridge that is
## defined elsewhere in this file. Uncomment and update if you want to use a
## different bridge for this component.
##
## data_gateway
## Required: The IP address that will be used as a next-hop address of the 
## data_static_route.
##
## data_static_route: list of eth1 static routes to the data networks
##
## ram (Gib)
## Optional: Ram amount that will be used for the VRS-VM. By default it is 4 (GB)
##
## vcpu
## Optional: VCPU count that will be used for the VRS-VM. By default it is 2
##
## vrs_vm_qcow2_path
## Required: source qcow path of the VRS-VM
#vrs_vm_operations_list:
#  - install
#
#myvrs_vms:
#  - { hostname: vrs_vm1,
#      #vmname: vrs_vm1,
#      target_server_type: kvm, 
#      target_server: 10.10.13.5,
#      ram: 12, 
#      vcpu: 4, 
#      mgmt_bridge: br0, 
#      mgmt_ip: 10.10.13.11,
#      mgmt_gateway: 10.10.13.1,
#      mgmt_netmask: 255.255.255.0, 
#      data_bridge: br1, 
#      data_ip: 10.9.13.11,
#      data_netmask: 255.255.255.0,
#      vrs_vm_qcow2_path: /tmp/images/centos7.qcow2,
#      data_gateway: 10.9.13.1,
#      data_static_route: [ 10.13.60.0/24, 10.12.60.0/24, 10.11.60.0/24] }
